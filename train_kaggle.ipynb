{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# ‚ôüÔ∏è Global Chess Challenge 2025: Qwen2.5-7B Training\n",
                "\n",
                "This notebook trains a **7B parameter model** (Qwen 2.5) on the ChessExplained dataset.\n",
                "\n",
                "### üöÄ Setup\n",
                "1. **GPU**: Use 2x T4 or A100.\n",
                "2. **Internet**: Enabled.\n",
                "3. **Secrets**: Add `WANDB_API_KEY` (optional)."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# 1. Install Dependencies & Repository\n",
                "!pip install datasets transformers accelerate chess wandb jinja2 vllm\n",
                "!pip install git+https://github.com/AIcrowd/chess-env.git\n",
                "!apt-get update && apt-get install -y stockfish\n",
                "\n",
                "# Install Baselines Repo directly (This provides callbacks, helpers, etc.)\n",
                "!pip install git+https://github.com/ritwikareddykancharla/Global-Chess-Challenge-2025-Baselines.git"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# 2. prepare Data & Tokenizer (Dynamic Setup)\n",
                "from transformers import AutoTokenizer\n",
                "from datasets import load_dataset\n",
                "import chess\n",
                "import os\n",
                "\n",
                "MODEL_NAME = \"Qwen/Qwen2.5-7B-Instruct\"\n",
                "\n",
                "# A. Load Dataset from Hugging Face\n",
                "print(\"Loading dataset from Hugging Face...\")\n",
                "dataset = load_dataset(\n",
                "    \"parquet\", \n",
                "    data_files=\"https://huggingface.co/datasets/aicrowd/ChessExplained/resolve/main/ChessExplained_2500k_qwen3.parquet\",\n",
                "    split=\"train\"\n",
                ")\n",
                "\n",
                "# B. Build Custom Tokenizer for 7B Model\n",
                "print(\"Building tokenizer with special chess tokens...\")\n",
                "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME, trust_remote_code=True)\n",
                "\n",
                "# Add Special Tokens\n",
                "squares = [f\"<{chess.square_name(sq)}>\" for sq in chess.SQUARES]\n",
                "pieces = [\n",
                "    '<White_Pawn>', '<White_Knight>', '<White_Bishop>', \n",
                "    '<White_Rook>', '<White_Queen>', '<White_King>',\n",
                "    '<Black_Pawn>', '<Black_Knight>', '<Black_Bishop>',\n",
                "    '<Black_Rook>', '<Black_Queen>', '<Black_King>',\n",
                "    '<blank>'\n",
                "]\n",
                "tokenizer.add_special_tokens({'additional_special_tokens': squares + pieces})\n",
                "if tokenizer.pad_token is None:\n",
                "    tokenizer.pad_token = tokenizer.eos_token\n",
                "\n",
                "tokenizer.save_pretrained(\"chess_tokenizer_7b\")\n",
                "TOKENIZER_PATH = \"chess_tokenizer_7b\""
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# 3. Training Loop\n",
                "import torch\n",
                "from datasets import Dataset\n",
                "import wandb\n",
                "from transformers import (\n",
                "    AutoModelForCausalLM,\n",
                "    TrainingArguments,\n",
                "    Trainer,\n",
                "    DataCollatorForLanguageModeling,\n",
                ")\n",
                "# Import directly from installed package\n",
                "from chess_evaluation_callback import ChessLLMEvaluationCallback\n",
                "\n",
                "# Configuration\n",
                "OUTPUT_DIR = \"./trained_models/qwen7b_chess\"\n",
                "NUM_LINES_TO_LOAD = 500_000 # Start small, increase for full run\n",
                "MAX_LENGTH = 512\n",
                "EVAL_STEPS = 2000\n",
                "\n",
                "wandb.init(project=\"ChessLLM\", name=\"qwen2.5-7b-kaggle\")\n",
                "\n",
                "# Load Model\n",
                "model = AutoModelForCausalLM.from_pretrained(\n",
                "    MODEL_NAME,\n",
                "    torch_dtype=torch.bfloat16,\n",
                "    device_map=\"auto\",\n",
                "    attn_implementation=\"flash_attention_2\",\n",
                ")\n",
                "model.resize_token_embeddings(len(tokenizer))\n",
                "\n",
                "# Load Data (Already loaded in Step 2)\n",
                "dataset = dataset.select(range(NUM_LINES_TO_LOAD))\n",
                "tokenized_dataset = dataset.map(lambda x: tokenizer(x[\"text\"], truncation=True, max_length=MAX_LENGTH), batched=True)\n",
                "\n",
                "# Trainer\n",
                "training_args = TrainingArguments(\n",
                "    output_dir=OUTPUT_DIR,\n",
                "    per_device_train_batch_size=4,\n",
                "    gradient_accumulation_steps=4,\n",
                "    learning_rate=2e-5, # Low LR for 7B\n",
                "    bf16=True,\n",
                "    logging_steps=100,\n",
                "    save_steps=EVAL_STEPS,\n",
                "    report_to=\"wandb\",\n",
                "    num_train_epochs=1\n",
                ")\n",
                "\n",
                "evaluation_callback = ChessLLMEvaluationCallback(\n",
                "    model=model,\n",
                "    tokenizer=tokenizer,\n",
                "    checkpoint_dir=OUTPUT_DIR,\n",
                "    eval_every_n_steps=EVAL_STEPS,\n",
                "    output_dir=\"./eval_results\",\n",
                "    batch_size=4,\n",
                ")\n",
                "\n",
                "trainer = Trainer(\n",
                "    model=model,\n",
                "    args=training_args,\n",
                "    train_dataset=tokenized_dataset,\n",
                "    data_collator=DataCollatorForLanguageModeling(tokenizer, mlm=False),\n",
                "    callbacks=[evaluation_callback]\n",
                ")\n",
                "\n",
                "trainer.train()\n",
                "trainer.save_model(f\"{OUTPUT_DIR}/final\")\n",
                "tokenizer.save_pretrained(f\"{OUTPUT_DIR}/final\")"
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "Python 3",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.10.0"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 4
}