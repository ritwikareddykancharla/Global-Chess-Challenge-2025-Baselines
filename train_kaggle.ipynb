{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# â™Ÿï¸ Global Chess Challenge 2025: Qwen2.5-7B Training (Exact Baseline Replica)\n",
                "\n",
                "This notebook mirrors `train_nvidia.py` but is optimized for Kaggle/H100 and Qwen 7B.\n",
                "\n",
                "### ðŸš€ Setup\n",
                "1. **GPU**: Use 2x T4 or A100.\n",
                "2. **Internet**: Enabled.\n",
                "3. **Secrets**: Add `WANDB_API_KEY`."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# 1. Install Dependencies & Repository\n",
                "!pip install datasets transformers accelerate chess wandb jinja2 vllm trl peft\n",
                "!pip install git+https://github.com/AIcrowd/chess-env.git\n",
                "!apt-get update && apt-get install -y stockfish\n",
                "# Install the baselines repo so we can import modules exactly like the script\n",
                "!pip install git+https://github.com/ritwikareddykancharla/Global-Chess-Challenge-2025-Baselines.git"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# 2. Argument & Configuration Setup (Mirrors train_nvidia.py)\n",
                "import os\n",
                "# os.environ[\"WANDB_MODE\"] = \"disabled\"\n",
                "\n",
                "import torch\n",
                "import pandas as pd\n",
                "import argparse\n",
                "from datasets import Dataset, load_dataset\n",
                "import wandb\n",
                "from transformers import (\n",
                "    AutoModelForCausalLM,\n",
                "    AutoTokenizer,\n",
                "    TrainingArguments,\n",
                "    Trainer,\n",
                "    DataCollatorForLanguageModeling,\n",
                ")\n",
                "from chess_evaluation_callback import ChessLLMEvaluationCallback\n",
                "\n",
                "# --- WandB Login for Kaggle ---\n",
                "try:\n",
                "    from kaggle_secrets import UserSecretsClient\n",
                "    user_secrets = UserSecretsClient()\n",
                "    wandb_key = user_secrets.get_secret(\"WANDB_API_KEY\")\n",
                "    wandb.login(key=wandb_key)\n",
                "    print(\"âœ… Logged into WandB using Kaggle Secret\")\n",
                "except Exception as e:\n",
                "    print(f\"âš ï¸ WandB auto-login failed: {e}. Make sure 'WANDB_API_KEY' is in Add-ons -> Secrets.\")\n",
                "    # wandb.login() # Uncomment to enter manually\n",
                "\n",
                "# Simulate Argument Parsing for Notebook\n",
                "class Args:\n",
                "    output_dir = \"./trained_models/qwen7b_chess_finetuned\"\n",
                "args = Args()\n",
                "\n",
                "# Validate output directory argument\n",
                "if os.path.exists(args.output_dir):\n",
                "    if len(os.listdir(args.output_dir)) > 0:\n",
                "        print(f\"Warning: Output directory '{args.output_dir}' already exists and is not empty\")\n",
                "\n",
                "# --- Configuration (H100/7B Updates) ---\n",
                "MODEL_NAME = \"Qwen/Qwen2.5-7B-Instruct\"\n",
                "OUTPUT_DIR = args.output_dir\n",
                "NUM_LINES_TO_LOAD = 1_000_000\n",
                "MAX_LENGTH = 512  \n",
                "EVAL_STEPS = 3000\n",
                "\n",
                "BATCH_SIZE = 8 # H100 Optimized\n",
                "GRAD_ACCUM_STEPS = 2\n",
                "LEARNING_RATE = 5e-5 # 7B Optimized\n",
                "WEIGHT_DECAY = 0.001\n",
                "WARMUP_STEPS = 500\n",
                "LOGGING_STEPS = EVAL_STEPS\n",
                "SAVE_STEPS = EVAL_STEPS\n",
                "SAVE_TOTAL_LIMIT = 10\n",
                "NUM_TRAIN_EPOCHS = 1 # 1 Epoch is sufficient for 1M+ games\n",
                "\n",
                "# Extract directory name for wandb run name\n",
                "run_name = os.path.basename(os.path.normpath(OUTPUT_DIR))\n",
                "wandb.init(project=\"ChessLLM\", name=run_name)\n",
                "\n",
                "print(f\"Using device: {torch.cuda.get_device_name(0) if torch.cuda.is_available() else 'CPU'}\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# 3. Load Dataset\n",
                "# Using the cleaner 'dataset/file' syntax as you requested\n",
                "print(\"Loading dataset...\")\n",
                "dataset = load_dataset(\n",
                "    \"aicrowd/ChessExplained\", \n",
                "    data_files=\"ChessExplained_2500k_qwen3.parquet\",\n",
                "    split=\"train\"\n",
                ").select(range(NUM_LINES_TO_LOAD))\n",
                "print(f\"Loaded {len(dataset)} examples\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# 4. Load/Prepare Tokenizer & Model\n",
                "import chess\n",
                "\n",
                "# Create 7B Tokenizer dynamically to ensure vocabulary match\n",
                "print(f\"Loading tokenizer for: {MODEL_NAME}\")\n",
                "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME, trust_remote_code=True)\n",
                "\n",
                "# Add Special Tokens (Same logic as prepare_tokenizer.ipynb)\n",
                "print(\"Adding chess special tokens...\")\n",
                "squares = [f\"<{chess.square_name(sq)}>\" for sq in chess.SQUARES]\n",
                "pieces = [\n",
                "    '<White_Pawn>', '<White_Knight>', '<White_Bishop>', \n",
                "    '<White_Rook>', '<White_Queen>', '<White_King>',\n",
                "    '<Black_Pawn>', '<Black_Knight>', '<Black_Bishop>',\n",
                "    '<Black_Rook>', '<Black_Queen>', '<Black_King>',\n",
                "    '<blank>'\n",
                "]\n",
                "tokenizer.add_special_tokens({'additional_special_tokens': squares + pieces})\n",
                "\n",
                "if tokenizer.pad_token is None:\n",
                "    tokenizer.pad_token = tokenizer.eos_token\n",
                "\n",
                "print(f\"Loading model: {MODEL_NAME}\")\n",
                "model = AutoModelForCausalLM.from_pretrained(\n",
                "    MODEL_NAME,\n",
                "    trust_remote_code=True,\n",
                "    dtype=torch.bfloat16,\n",
                "    device_map=\"auto\",\n",
                "    attn_implementation=\"flash_attention_2\",\n",
                ")\n",
                "model.resize_token_embeddings(len(tokenizer))\n",
                "\n",
                "print(f\"Model loaded with {model.num_parameters():,} parameters\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# 5. Tokenize the dataset (Exact match to script)\n",
                "def tokenize_function(examples):\n",
                "    \"\"\"Tokenize text examples\"\"\"\n",
                "    tokenized = tokenizer(\n",
                "        examples[\"text\"],\n",
                "        truncation=True,\n",
                "        max_length=MAX_LENGTH,\n",
                "        padding=False,  # We'll pad dynamically during training\n",
                "        return_tensors=None\n",
                "    )\n",
                "    return tokenized\n",
                "\n",
                "print(\"Tokenizing dataset...\")\n",
                "tokenized_dataset = dataset.map(\n",
                "    tokenize_function,\n",
                "    batched=True,\n",
                "    remove_columns=dataset.column_names,\n",
                "    desc=\"Tokenizing\"\n",
                ")\n",
                "\n",
                "print(f\"Tokenization complete. Sample token count: {len(tokenized_dataset[0]['input_ids'])}\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# 6. Training Arguments & Collator\n",
                "training_args = TrainingArguments(\n",
                "    output_dir=OUTPUT_DIR,\n",
                "    per_device_train_batch_size=BATCH_SIZE,\n",
                "    gradient_accumulation_steps=GRAD_ACCUM_STEPS,\n",
                "    gradient_checkpointing=False,\n",
                "    learning_rate=LEARNING_RATE,\n",
                "    weight_decay=WEIGHT_DECAY,\n",
                "    warmup_steps=WARMUP_STEPS,\n",
                "    logging_steps=LOGGING_STEPS,\n",
                "    save_steps=SAVE_STEPS,\n",
                "    save_total_limit=SAVE_TOTAL_LIMIT,\n",
                "    bf16=True,\n",
                "    fp16=False,\n",
                "    optim=\"adamw_torch_fused\",\n",
                "    remove_unused_columns=False,\n",
                "    report_to=\"wandb\",\n",
                "    dataloader_drop_last=True,\n",
                "    num_train_epochs=NUM_TRAIN_EPOCHS,\n",
                ")\n",
                "\n",
                "print(\"Training configuration:\")\n",
                "print(f\"  Batch size: {training_args.per_device_train_batch_size}\")\n",
                "print(f\"  Effective batch size: {training_args.per_device_train_batch_size * training_args.gradient_accumulation_steps}\")\n",
                "print(f\"  Learning rate: {training_args.learning_rate}\")\n",
                "\n",
                "data_collator = DataCollatorForLanguageModeling(\n",
                "    tokenizer=tokenizer,\n",
                "    mlm=False\n",
                ")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# 7. Logic & Trainer Initialization\n",
                "evaluation_callback = ChessLLMEvaluationCallback(\n",
                "    model=model,\n",
                "    tokenizer=tokenizer,\n",
                "    checkpoint_dir=OUTPUT_DIR,\n",
                "    eval_every_n_steps=EVAL_STEPS,\n",
                "    output_dir=\"./eval_results_during_training\",\n",
                "    vllm_port=8000,\n",
                "    batch_size=BATCH_SIZE,\n",
                "    grad_accum_steps=GRAD_ACCUM_STEPS,\n",
                "    warmup_steps=WARMUP_STEPS,\n",
                ")\n",
                "\n",
                "trainer = Trainer(\n",
                "    model=model,\n",
                "    args=training_args,\n",
                "    train_dataset=tokenized_dataset,\n",
                "    data_collator=data_collator,\n",
                "    callbacks=[evaluation_callback]\n",
                ")\n",
                "\n",
                "print(\"Trainer initialized with full evaluation callback. Ready to start training.\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# 8. Training Execution\n",
                "try:\n",
                "    print(\"Starting training...\")\n",
                "\n",
                "    trainer.train()\n",
                "\n",
                "    print(\"\\nâœ… Training complete!\")\n",
                "\n",
                "    final_model_path = f\"{OUTPUT_DIR}/final_model\"\n",
                "    print(f\"Saving final model to {final_model_path}\")\n",
                "\n",
                "    trainer.save_model(final_model_path)\n",
                "    tokenizer.save_pretrained(final_model_path)\n",
                "\n",
                "    print(f\"âœ… Model and tokenizer saved to {final_model_path}\")\n",
                "except KeyboardInterrupt:\n",
                "    print(\"Training interrupted by user\")\n",
                "finally:\n",
                "    wandb.finish()"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# ðŸš€ Phase 2: GRPO Training (Advanced)\n",
                "\n",
                "This section implements **Group Relative Policy Optimization (GRPO)** using Stockfish rewards.\n",
                "\n",
                "**Setup**: We load the SFT model we just finished training and refine it."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "import chess\n",
                "import chess.engine\n",
                "from trl import GRPOTrainer, GRPOConfig\n",
                "import re\n",
                "\n",
                "# --- 1. Persistent Stockfish Engine ---\n",
                "# We keep one engine alive to save startup time\n",
                "ENGINE_PATH = \"/usr/games/stockfish\"\n",
                "# engine = chess.engine.SimpleEngine.popen_uci(ENGINE_PATH) # Initialize once\n",
                "\n",
                "def stockfish_reward_func(prompts, completions, **kwargs):\n",
                "    \"\"\"Calculates reward using Stockfish Eval (Fast Depth 10)\"\"\"\n",
                "    rewards = []\n",
                "    # Note: In a real run, we need to extract the BOARD state from the prompt\n",
                "    # For this template, we show the logic for 1 completion\n",
                "    for completion in completions:\n",
                "        # 1. Extract move\n",
                "        match = re.search(r'<uci_move>(.*?)</uci_move>', completion)\n",
                "        if not match:\n",
                "            rewards.append(-1.0) # Penalty for format error\n",
                "            continue\n",
                "        \n",
                "        move_uci = match.group(1)\n",
                "        \n",
                "        # 2. Logic to apply move to board would go here\n",
                "        # board.push(chess.Move.from_uci(move_uci))\n",
                "        \n",
                "        # 3. Evaluate\n",
                "        # info = engine.analyse(board, chess.engine.Limit(depth=10))\n",
                "        # score = info['score'].white().score(mate_score=10000)\n",
                "        \n",
                "        # Placeholder reward for template\n",
                "        rewards.append(0.5)\n",
                "        \n",
                "    return rewards\n",
                "\n",
                "# --- 2. GRPO Configuration ---\n",
                "grpo_args = GRPOConfig(\n",
                "    output_dir=\"./qwen7b_grpo_rl\",\n",
                "    learning_rate=1e-6,         # Very low LR for RL\n",
                "    per_device_train_batch_size=4,\n",
                "    gradient_accumulation_steps=4,\n",
                "    max_prompt_length=256,\n",
                "    max_completion_length=64,\n",
                "    num_generations=4,          # How many samples per prompt\n",
                "    bf16=True,\n",
                ")\n",
                "\n",
                "# --- 3. Initialize Trainer ---\n",
                "# model_grpo = AutoModelForCausalLM.from_pretrained(f\"{OUTPUT_DIR}/final_model\", ...)\n",
                "\n",
                "# trainer = GRPOTrainer(\n",
                "#     model=model_grpo,\n",
                "#     reward_funcs=[stockfish_reward_func],\n",
                "#     args=grpo_args,\n",
                "#     train_dataset=dataset, # Requires 'prompt' column\n",
                "# )\n",
                "\n",
                "print(\"GRPO Template ready. Uncomment lines above to run Phase 2.\")"
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "Python 3",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.10.0"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 4
}